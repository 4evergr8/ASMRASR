import gc
import torch
import numpy as np
from pyannote.audio import Model
import os
from faster_whisper import WhisperModel
import librosa
from pyannote.audio.pipelines import VoiceActivityDetection
from dataclasses import dataclass

import yaml
import os

# 配置
path = '/content/gdrive/MyDrive/ASR'
config = {
    "work_path": "1work",
    "asr_path": "1work",
    "log_path": "log",
    "model_path": "model",

    "prompt": "",
    "language": "ja",
    "space": 3,
    "min_duration_on": 0.0,
    "min_duration_off": 0.2,

    "asr": "large-v2",
    "vad": "4evergr8/pyannote-segmentation-3.0",

    "output": ["lrc", "srt", "vtt"]
}

config["work_path"] = os.path.join(path, config["work_path"])
config["asr_path"] = os.path.join(path, config["asr_path"])

# 数据结构
@dataclass
class AudioSegmentInfo:
    start_in_concat: float
    end_in_concat: float
    start_in_origin: float
    end_in_origin: float

@dataclass
class AudioSegmentGroup:
    audio: np.ndarray
    segments: list

# 工具函数
def timestamp_to_srt(ts: float) -> str:
    hours = int(ts // 3600)
    minutes = int((ts % 3600) // 60)
    seconds = int(ts % 60)
    millis = int((ts - int(ts)) * 1000)
    return f"{hours:02}:{minutes:02}:{seconds:02},{millis:03}"

def map_back_time(time_in_concat, segments):
    for seg in segments:
        if seg.start_in_concat <= time_in_concat <= seg.end_in_concat:
            relative = time_in_concat - seg.start_in_concat
            return seg.start_in_origin + relative
    return segments[-1].end_in_origin


# 硬件
device = "cuda" if torch.cuda.is_available() else "cpu"
compute_type = "float16" if device == "cuda" else "int8"
print('设备:', device, '类型:', compute_type)

# 只初始化一次ASR模型（不会在每个音频内循环初始化）
asr_model = WhisperModel(
    config["asr"],
    device=device,
    compute_type=compute_type,
    download_root=config["model_path"]
)

# 遍历所有音频
for root, dirs, files in os.walk(config["work_path"]):
    for filename in files:
        if not filename.endswith(('.wav', '.mp3')):
            continue

        audio_path = os.path.join(root, filename)
        basename = os.path.splitext(filename)[0]
        print(f"\n处理音频: {audio_path}")

        # Step 1: 加载音频
        audio, sr = librosa.load(audio_path, sr=16000, mono=True)

        # Step 2: VAD推理
        gc.collect()
        torch.cuda.empty_cache()

        vad_model = Model.from_pretrained(checkpoint=config["vad"], cache_dir=config["model_path"])
        vad_model.to(torch.device(device))
        vad_pipeline = VoiceActivityDetection(segmentation=vad_model)
        vad_pipeline.instantiate({
            "min_duration_on": config["min_duration_on"],
            "min_duration_off": config["min_duration_off"],
        })

        vad_result = vad_pipeline(str(audio_path))

        # VAD结束，释放模型和缓存
        del vad_pipeline, vad_model
        gc.collect()
        torch.cuda.empty_cache()

        # Step 3: 切人声片段
        timeline = vad_result.get_timeline()
        silence = np.zeros(int(sr * config["space"]), dtype=audio.dtype)

        groups = []
        current_audio = []
        current_segments = []
        current_time_concat = 0.0
        group_start_limit = 30 * 60  # 30分钟

        for segment in timeline:
            if segment.start >= group_start_limit:
                break

            start_sample = int(segment.start * sr)
            end_sample = int(segment.end * sr)

            audio_seg = audio[start_sample:end_sample]
            duration = (end_sample - start_sample) / sr

            segment_info = AudioSegmentInfo(
                start_in_concat=current_time_concat,
                end_in_concat=current_time_concat + duration,
                start_in_origin=segment.start,
                end_in_origin=segment.end
            )

            current_segments.append(segment_info)
            current_audio.append(audio_seg)
            current_audio.append(silence)
            current_time_concat += duration + config["space"]

        if current_segments:
            final_audio = np.concatenate(current_audio[:-1])  # 去掉最后一个silence
            groups.append(AudioSegmentGroup(audio=final_audio, segments=current_segments))

        if not groups:
            print(f"{audio_path} 没有有效人声段，跳过")
            continue

        # Step 4: ASR转写
        srt_segments = []
        subtitle_index = 1

        for group in groups:
            segments, _ = asr_model.transcribe(
                audio=group.audio,
                beam_size=2,
                vad_filter=False,
                initial_prompt=basename,
                language=config['language'],
                word_timestamps=False
            )

            for seg in segments:
                start_real = map_back_time(seg.start, group.segments)
                end_real = map_back_time(seg.end, group.segments)

                srt_segments.append((subtitle_index, start_real, end_real, seg.text.strip()))
                subtitle_index += 1

        # Step 5: 写出srt
        os.makedirs(config["asr_path"], exist_ok=True)
        srt_path = os.path.join(config["asr_path"], f"{basename}.srt")
        with open(srt_path, "w", encoding="utf-8") as f:
            for idx, start, end, text in srt_segments:
                f.write(f"{idx}\n")
                f.write(f"{timestamp_to_srt(start)} --> {timestamp_to_srt(end)}\n")
                f.write(f"{text}\n\n")
        print(f"字幕写入: {srt_path}")

        # 处理完一个音频彻底回收
        del audio, groups, timeline, srt_segments
        gc.collect()
        torch.cuda.empty_cache()

# 处理结束后，释放ASR模型
del asr_model
gc.collect()
torch.cuda.empty_cache()




不对，每一段0到30分钟都要额外计算这一段中起始人声的原坐标