{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1下载文件"
  },
  {
   "cell_type": "code",
   "source": [
    "import gdown\n",
    "import os\n",
    "\n",
    "def download_from_gdrive(url: str, output_dir: str, filename: str):\n",
    "    file_id = url.split('/d/')[1].split('/')[0]\n",
    "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    gdown.download(download_url, output=output_path, quiet=False, fuzzy=True)\n",
    "output_dir = '/kaggle/working/ASMRASR/b_work'\n",
    "download_from_gdrive(\n",
    "    \"https://drive.google.com/file/d/1BAdZZFu_aYTFVnIbyjDqAFCFF5OLSWqB/view?usp=sharing\",\n",
    "    output_dir,\n",
    "    \"asmr_data.wav\"\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "id": "L9swbMgOLIIG",
    "cellView": "form",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-09T11:33:18.439472Z",
     "iopub.execute_input": "2025-05-09T11:33:18.439810Z",
     "iopub.status.idle": "2025-05-09T11:33:28.654565Z",
     "shell.execute_reply.started": "2025-05-09T11:33:18.439784Z",
     "shell.execute_reply": "2025-05-09T11:33:28.653855Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2安装依赖"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pysrt\n",
    "!pip install pyannote.audio\n",
    "!pip install faster-whisper==1.1.1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "J62U5RDnLIIM",
    "cellView": "form",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-09T11:06:59.749046Z",
     "iopub.execute_input": "2025-05-09T11:06:59.749307Z",
     "iopub.status.idle": "2025-05-09T11:07:09.622282Z",
     "shell.execute_reply.started": "2025-05-09T11:06:59.749286Z",
     "shell.execute_reply": "2025-05-09T11:07:09.621442Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3导入配置"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "config = {\n",
    "    \"pre_path\": \"a_pre\",\n",
    "    \"work_path\": \"b_work\",\n",
    "    \"asr_path\": \"c_asr\",\n",
    "    \"tsl_path\": \"d_tsl\",\n",
    "    \"model_path\": \"/kaggle/working/model\",\n",
    "    \"log_path\": \"z_log\",\n",
    "    \"overwrite_vad\": False,\n",
    "    \"overwrite_asr\": True,\n",
    "    \"separator\": \"MDX23C-8KFFT-InstVoc_HQ_2.ckpt\",\n",
    "    \"vad\": \"4evergr8/pyannote-segmentation-3.0\",\n",
    "    \"asr\": \"large-v3-turbo\",\n",
    "    \"translate\": \"gemini-1.5-flash\",\n",
    "    \"batch_size\": 8,\n",
    "    \"min_duration_on\": 0.0,\n",
    "    \"min_duration_off\": 0.2,\n",
    "    \"space\": 2,\n",
    "    \"language\": \"ja\",\n",
    "    \"num_workers\": 15,\n",
    "    \"api_key\": \"\",\n",
    "    \"prompt\": \"\"\"你是一名专业的翻译家，请你按照以下流程进行翻译：\n",
    "第一步：初步直译\n",
    "将日语文本逐行直译成简体中文文本，保留序号、标记符、占位符、换行符等特殊内容，保持原来的格式。\n",
    "\n",
    "第二步：深入校正\n",
    "针对每一句初步译文，可以从语义与语境、专业术语、上下文信息、翻译风格、故事背景、人物设定等等方面出发，进行深入分析和校正。\n",
    "\n",
    "第三步：最终意译与润色\n",
    "整合直译结果和校正建议，进行最终的意译和润色，生成自然流畅、符合简体中文表达习惯的最终译文。\n",
    "\n",
    "### 翻译原则\n",
    "忠实准确地进行翻译。原作是伟大艺术作品，允许使用直白或粗俗的描述，不应随意删减或篡改，不应回避或淡化，以保持作品的原貌。\n",
    "只输出最终的翻译结果，不输出中间结果，结合上下文进行翻译，注意用“|”分割的序号，我会先将原文发给你用来熟悉上下文，然后分批次获取翻译内容，原文如下：\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_path(config_value):\n",
    "    path = os.path.join('/kaggle/working/ASMRASR', config_value)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(f\"创建文件夹: {path}\")\n",
    "    return path\n",
    "\n",
    "config[\"pre_path\"] = get_path(config[\"pre_path\"])\n",
    "config[\"work_path\"] = get_path(config[\"work_path\"])\n",
    "config[\"asr_path\"] = get_path(config[\"asr_path\"])\n",
    "config[\"tsl_path\"] = get_path(config[\"tsl_path\"])\n",
    "config[\"log_path\"] = get_path(config[\"log_path\"])"
   ],
   "metadata": {
    "id": "krTHocPaLIIO",
    "cellView": "form",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-09T11:17:50.555038Z",
     "iopub.execute_input": "2025-05-09T11:17:50.555299Z",
     "iopub.status.idle": "2025-05-09T11:17:50.566853Z",
     "shell.execute_reply.started": "2025-05-09T11:17:50.555275Z",
     "shell.execute_reply": "2025-05-09T11:17:50.566263Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 人声分离"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from imageio_ffmpeg import get_ffmpeg_exe\n",
    "from audio_separator.separator import Separator\n",
    "\n",
    "ffmpeg = get_ffmpeg_exe()\n",
    "os.system(\"chcp 65001\")\n",
    "\n",
    "\n",
    "for audio_filename in os.listdir(config[\"pre_path\"]):\n",
    "    if audio_filename.endswith((\".wav\", \".mp3\", \".flac\")):\n",
    "        basename = os.path.splitext(audio_filename)[0]\n",
    "        slice_path = os.path.join(config[\"pre_path\"], f\"{basename}-slice\")\n",
    "        split_path = os.path.join(config[\"pre_path\"], f\"{basename}-split\")\n",
    "        audio_path = os.path.join(config[\"pre_path\"], audio_filename)\n",
    "        if os.path.isfile(audio_path):\n",
    "\n",
    "            if os.path.exists(slice_path):\n",
    "                shutil.rmtree(slice_path)  # 删除原文件夹及其内容\n",
    "            os.makedirs(slice_path)  # 创建新文件夹\n",
    "\n",
    "            segment_length = 1200  # 20 分钟 = 1200 秒\n",
    "            command = [\n",
    "                ffmpeg, \"-i\", audio_path,  # 输入音频文件\n",
    "                \"-f\", \"segment\",  # 使用 segment 格式进行切割\n",
    "                \"-segment_time\", str(segment_length),  # 设置每段的时长（单位：秒）\n",
    "                \"-c\", \"copy\",  # 保持原始编码（无损切割）\n",
    "                os.path.join(slice_path, \"%03d.wav\")  # 输出文件的命名格式\n",
    "            ]\n",
    "            subprocess.run(command, check=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            separator = Separator(\n",
    "                model_file_dir=config[\"model_path\"],\n",
    "                output_dir=split_path,\n",
    "                output_single_stem=\"vocals\",\n",
    "                sample_rate=16000,\n",
    "                mdxc_params={\"segment_size\": 256, \"override_model_segment_size\": False, \"batch_size\": config[\"batch_size\"],\n",
    "                             \"overlap\": 8, \"pitch_shift\": 0}\n",
    "            )\n",
    "            separator.load_model(model_filename=config[\"separator\"])\n",
    "            for filename in os.listdir(slice_path):\n",
    "                if filename.endswith(\".wav\"):\n",
    "                    slice_basename = os.path.splitext(filename)[0]  # 比如 '001'\n",
    "                    exists = any(name.startswith(slice_basename) for name in os.listdir(split_path))\n",
    "                    if not exists:\n",
    "                        output_files = separator.separate(os.path.join(slice_path, filename))\n",
    "                        print(f\"<UNK>{len(output_files)}\")\n",
    "                    else:\n",
    "                        print(f\"已存在分离结果，跳过：{filename}\")\n",
    "\n",
    "            file_list = sorted(\n",
    "                [f for f in os.listdir(split_path) if f.endswith(\".wav\")],\n",
    "                key=lambda x: int(x[:3])\n",
    "            )\n",
    "            with open(os.path.join(config[\"pre_path\"], f\"{basename}_list.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                for f_name in file_list:\n",
    "                    full_path = os.path.join(split_path, f_name)\n",
    "                    f.write(f\"file '{full_path}'\\n\")\n",
    "\n",
    "            basename = os.path.splitext(audio_filename)[0]\n",
    "            output_path = os.path.join(config[\"work_path\"], f\"{basename}.wav\")\n",
    "            command = [\n",
    "                ffmpeg,\n",
    "                \"-f\", \"concat\",\n",
    "                \"-safe\", \"0\",\n",
    "                \"-i\", os.path.join(config[\"pre_path\"], f\"{basename}_list.txt\"),\n",
    "                \"-c\", \"copy\",\n",
    "                output_path\n",
    "            ]\n",
    "            subprocess.run(command, check=True)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "PYephQIxLIIQ",
    "cellView": "form",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-09T08:20:48.370633Z",
     "iopub.execute_input": "2025-05-09T08:20:48.371105Z",
     "iopub.status.idle": "2025-05-09T08:20:48.408753Z",
     "shell.execute_reply.started": "2025-05-09T08:20:48.371064Z",
     "shell.execute_reply": "2025-05-09T08:20:48.406550Z"
    },
    "jupyter": {
     "source_hidden": true,
     "outputs_hidden": true
    },
    "collapsed": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 5音频转写\n",
    "import gc\n",
    "import pysrt\n",
    "import torch\n",
    "import numpy as np\n",
    "from pyannote.audio import Model\n",
    "from faster_whisper import WhisperModel\n",
    "import librosa\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "import os\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float32\" if device == \"cuda\" else \"int8\"\n",
    "print('设备:', device, '类型:', compute_type)\n",
    "\n",
    "for filename in os.listdir(config[\"work_path\"]):\n",
    "    if not filename.endswith((\".wav\", \".mp3\", \".flac\")):\n",
    "        continue\n",
    "    audio_path = os.path.join(config[\"work_path\"], filename)\n",
    "    print(f\"\\n处理音频: {audio_path}\")\n",
    "    basename = os.path.splitext(filename)[0]\n",
    "\n",
    "    vad_log_path = os.path.join(config[\"log_path\"], f\"vad-{basename}.srt\")\n",
    "    if not os.path.exists(vad_log_path) or config[\"overwrite_vad\"]:\n",
    "        vad_model = Model.from_pretrained(checkpoint=config[\"vad\"], cache_dir=config[\"model_path\"])\n",
    "        vad_model.to(torch.device(device))\n",
    "        vad_pipeline = VoiceActivityDetection(segmentation=vad_model)\n",
    "        vad_pipeline.instantiate({\n",
    "            \"min_duration_on\": config[\"min_duration_on\"],\n",
    "            \"min_duration_off\": config[\"min_duration_off\"],\n",
    "        })\n",
    "\n",
    "        vad_result = vad_pipeline(str(audio_path))\n",
    "        del vad_pipeline, vad_model\n",
    "\n",
    "        timeline = vad_result.get_timeline()\n",
    "        vad_log = pysrt.SubRipFile()\n",
    "\n",
    "        group_duration = 1800  # 每组时长：30分钟\n",
    "        for segment in timeline:\n",
    "            group_index = int(segment.end // group_duration)\n",
    "            group_base_idx = 1000 + group_index * 1000\n",
    "            sub_index = group_base_idx + len(\n",
    "                [s for s in vad_log if group_base_idx <= s.index < group_base_idx + 1000])\n",
    "\n",
    "            sub = pysrt.SubRipItem(\n",
    "                index=sub_index,\n",
    "                start=pysrt.SubRipTime.from_ordinal(int(segment.start * 1000)),\n",
    "                end=pysrt.SubRipTime.from_ordinal(int(segment.end * 1000)),\n",
    "                text=\"默认占位\" + str(sub_index)\n",
    "            )\n",
    "            vad_log.append(sub)\n",
    "\n",
    "        vad_log.save(vad_log_path)\n",
    "        print(f\"VAD记录写入: {vad_log_path}\")\n",
    "    else:\n",
    "        vad_log = pysrt.open(vad_log_path)\n",
    "        print('VAD记录存在，跳过')\n",
    "\n",
    "    slice_log = pysrt.SubRipFile()  # 用于存储调整后的字幕\n",
    "    silence_duration = config[\"space\"]  # 获取配置中的静音时间\n",
    "    current_group_end = 0.0  # 当前组的结束时间\n",
    "\n",
    "    for subtitle in vad_log:\n",
    "        segment_start = subtitle.start.ordinal / 1000  # 转换为秒\n",
    "        segment_end = subtitle.end.ordinal / 1000  # 转换为秒\n",
    "\n",
    "        # 如果是该组的第一个段（序号能被1000整除）\n",
    "        if subtitle.index % 1000 == 0:\n",
    "            group_start = 0.0  # 设置为0，确保每组的第一个段从00:00:00开始\n",
    "        else:\n",
    "            group_start = current_group_end  # 否则按照上一段的结束时间进行处理\n",
    "\n",
    "        group_end = group_start + (segment_end - segment_start)  # 保持时间段的长度不变\n",
    "\n",
    "        # 更新字幕的开始和结束时间戳\n",
    "        subtitle.start = pysrt.SubRipTime.from_ordinal(int(group_start * 1000))  # 转换为毫秒并设置新的开始时间\n",
    "        subtitle.end = pysrt.SubRipTime.from_ordinal(int(group_end * 1000))  # 设置新的结束时间\n",
    "\n",
    "        # 更新当前组的结束时间\n",
    "        current_group_end = group_end + silence_duration  # 下一组的开始时间是当前组的结束时间+静音间隔\n",
    "\n",
    "        # 将处理后的字幕项添加到slice_log中\n",
    "        slice_log.append(subtitle)\n",
    "\n",
    "    slice_log_path = os.path.join(config[\"log_path\"], f\"slice-{basename}.srt\")\n",
    "    slice_log.save(slice_log_path)\n",
    "    print(f\"slice记录写入: {slice_log_path}\")\n",
    "\n",
    "    asr_log_path = os.path.join(config[\"log_path\"], f\"asr-{basename}.srt\")\n",
    "    if not os.path.exists(asr_log_path) or config[\"overwrite_asr\"]:\n",
    "\n",
    "        # 加载模型\n",
    "        asr_model = WhisperModel(\n",
    "            model_size_or_path=config[\"asr\"],\n",
    "            device=device,\n",
    "            compute_type=compute_type,\n",
    "            download_root=config[\"model_path\"],\n",
    "            num_workers=config[\"num_workers\"]\n",
    "        )\n",
    "\n",
    "        # 初始化 ASR 记录\n",
    "        asr_log = pysrt.SubRipFile()\n",
    "        base_index = 1000  # 初始组编号起点\n",
    "\n",
    "        # 加载字幕文件并分组\n",
    "        vad_log = pysrt.open(vad_log_path)\n",
    "        group_dict = {}\n",
    "\n",
    "        for subtitle in vad_log:\n",
    "            segment_start = subtitle.start.ordinal / 1000  # 秒\n",
    "            segment_end = subtitle.end.ordinal / 1000  # 秒\n",
    "            index = subtitle.index\n",
    "            group_id = index // 1000  # 1000~1999 为第 1 组，2000~2999 为第 2 组，以此类推\n",
    "\n",
    "            if group_id not in group_dict:\n",
    "                group_dict[group_id] = []\n",
    "            group_dict[group_id].append((segment_start, segment_end))\n",
    "\n",
    "        # 对每一组进行处理\n",
    "        for group_id, segments in group_dict.items():\n",
    "            # 加载音频\n",
    "            audio, sr = librosa.load(str(audio_path), sr=16000, mono=True)\n",
    "\n",
    "            # 创建静音段\n",
    "            silence_duration = config[\"space\"]\n",
    "            silence = np.zeros(int(sr * silence_duration), dtype=np.float32)\n",
    "\n",
    "            # 切分并拼接该组的音频\n",
    "            group_audio = []\n",
    "            for i, (start, end) in enumerate(segments):\n",
    "                start_sample = int(start * sr)\n",
    "                end_sample = int(end * sr)\n",
    "                audio_seg = audio[start_sample:end_sample]\n",
    "\n",
    "                if i > 0:\n",
    "                    group_audio.append(silence)  # 插入静音间隔\n",
    "                group_audio.append(audio_seg)\n",
    "\n",
    "            group_array = np.concatenate(group_audio) if group_audio else np.array([], dtype=np.float32)\n",
    "            del audio  # 删除音频，释放内存\n",
    "\n",
    "            # 送入模型进行识别\n",
    "            segments, _ = asr_model.transcribe(\n",
    "                audio=group_array,\n",
    "                language=config['language'],\n",
    "                task=\"transcribe\",\n",
    "                log_progress=True,\n",
    "                beam_size=5,\n",
    "                best_of=5,\n",
    "                patience=1,\n",
    "                length_penalty=1,\n",
    "                repetition_penalty=1.1,\n",
    "                no_repeat_ngram_size=3,\n",
    "                temperature=[0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                compression_ratio_threshold=2.4,\n",
    "                log_prob_threshold=-1.0,\n",
    "                no_speech_threshold=0.6,\n",
    "                condition_on_previous_text=True,\n",
    "                prompt_reset_on_temperature=0.5,\n",
    "                prefix=None,\n",
    "                suppress_blank=True,\n",
    "                suppress_tokens=[-1],\n",
    "                without_timestamps=False,\n",
    "                max_initial_timestamp=1.0,\n",
    "                word_timestamps=True,\n",
    "                hallucination_silence_threshold=1.5,\n",
    "                prepend_punctuations=\"\\\"'“¿([{-\",\n",
    "                append_punctuations=\"\\\"'.。,，!！?？:：”)]}、\",\n",
    "                multilingual=False,\n",
    "                vad_filter=False,\n",
    "                clip_timestamps=\"0\",\n",
    "                language_detection_threshold=None,\n",
    "                language_detection_segments=1,\n",
    "                hotwords='イッたんだよ やだ まって…やばい… 恥ずかしい'\n",
    "            )\n",
    "\n",
    "            # 添加识别结果到 ASR 记录并写入文件\n",
    "            for i, seg in enumerate(segments):\n",
    "                seg_start = seg.start\n",
    "                seg_end = seg.end\n",
    "                seg_text = seg.text.strip()\n",
    "\n",
    "                subtitle = pysrt.SubRipItem(\n",
    "                    index=base_index + i,\n",
    "                    start=pysrt.SubRipTime.from_ordinal(int(seg_start * 1000)),\n",
    "                    end=pysrt.SubRipTime.from_ordinal(int(seg_end * 1000)),\n",
    "                    text=seg_text\n",
    "                )\n",
    "                asr_log.append(subtitle)\n",
    "\n",
    "            asr_log.save(asr_log_path)\n",
    "            print(f\"组 {group_id} 的识别结果已写入: {asr_log_path}\")\n",
    "\n",
    "        # 删除模型\n",
    "        del asr_model\n",
    "        print(\"模型已删除\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        asr_log = pysrt.open(asr_log_path)\n",
    "        print('ASR记录存在，跳过')\n",
    "\n",
    "    for slice_sub in slice_log:\n",
    "        segment_start = slice_sub.start.ordinal / 1000\n",
    "        segment_end = slice_sub.end.ordinal / 1000\n",
    "        segment_index = slice_sub.index\n",
    "        group_prefix = (segment_index // 1000) * 1000\n",
    "\n",
    "        max_overlap = 0.0\n",
    "        best_match = None\n",
    "\n",
    "        # 查找当前组中所有 asr_log 字幕（同一千段内）\n",
    "        for asr_sub in asr_log:\n",
    "            if (asr_sub.index // 1000) * 1000 != group_prefix:\n",
    "                continue\n",
    "\n",
    "            seg_start = asr_sub.start.ordinal / 1000\n",
    "            seg_end = asr_sub.end.ordinal / 1000\n",
    "\n",
    "            # 重合检测\n",
    "            overlap_start = max(seg_start, segment_start)\n",
    "            overlap_end = min(seg_end, segment_end)\n",
    "            overlap_duration = overlap_end - overlap_start\n",
    "\n",
    "            if overlap_duration > max_overlap:\n",
    "                max_overlap = overlap_duration\n",
    "                best_match = asr_sub\n",
    "\n",
    "        if best_match is not None:\n",
    "            slice_sub.text = best_match.text\n",
    "\n",
    "    # 保存更新后的 slice_log\n",
    "    match_path = os.path.join(config[\"log_path\"], f\"match-{basename}.srt\")\n",
    "    slice_log.save(match_path)\n",
    "    print(f\"match结果写入: {match_path}\")\n",
    "\n",
    "    vad_log = pysrt.open(vad_log_path)\n",
    "\n",
    "    for vad_sub in vad_log:\n",
    "        for slice_sub in slice_log:\n",
    "            idx = slice_sub.index\n",
    "\n",
    "            if vad_sub.index == idx:\n",
    "                vad_sub.text = slice_sub.text\n",
    "                break  # 找到对应的字幕后可以停止循环，避免多次匹配\n",
    "\n",
    "    for i, sub in enumerate(vad_log, 1):\n",
    "        sub.index = i\n",
    "\n",
    "    result_path = os.path.join(config[\"asr_path\"], f\"{basename}.srt\")\n",
    "    vad_log.save(result_path)\n",
    "    print(f\"结果写入: {result_path}\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "0t_AN__qLIIS",
    "cellView": "form",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-09T12:05:11.283615Z",
     "iopub.execute_input": "2025-05-09T12:05:11.284050Z",
     "iopub.status.idle": "2025-05-09T12:10:51.499420Z",
     "shell.execute_reply.started": "2025-05-09T12:05:11.284017Z",
     "shell.execute_reply": "2025-05-09T12:10:51.498333Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#@title 6结果翻译\nimport os\nimport time\nimport pysrt\nfrom google import genai\nfrom google.genai import types\n\n\n\nfor filename in os.listdir(config[\"asr_path\"]):\n    if not filename.endswith(\".srt\"):\n        continue\n    basename = os.path.splitext(filename)[0]\n    src_path = os.path.join(config[\"asr_path\"], filename)\n    dst_path = os.path.join(config[\"tsl_path\"], filename)\n\n    original_subs = pysrt.open(src_path, encoding='utf-8')\n\n    # 创建新的 SubRipFile 并重新编号\n    original_subs = pysrt.SubRipFile(items=[sub for sub in original_subs if \"默认占位\" not in sub.text])\n\n\n    original_subs.clean_indexes()\n\n\n    try:\n        translated_subs = pysrt.open(dst_path, encoding='utf-8')\n    except:\n        translated_subs = pysrt.SubRipFile()\n\n    # 如果翻译的字幕与原字幕一样长，跳过\n    if len(translated_subs) == len(original_subs):\n        print(f\"文件 {filename} 已经全部翻译完，跳过.\")\n        continue\n\n    # 计算剩余未翻译的部分\n    original_subs = original_subs[len(translated_subs):]\n\n    # 翻译剩余部分\n    for i in range(0, len(original_subs), 200):\n        chunk = original_subs[i:i + 200]\n\n        client = genai.Client(api_key=config[\"api_key\"])\n        chat = client.chats.create(\n            model=config['translate'],\n            config=types.GenerateContentConfig(\n                system_instruction=config[\"prompt\"] + \"\\n\" + \"\\n\".join(\n                    sub.text for sub in chunk),\n            )\n        )\n\n        for j in range(0, len(chunk), 10):\n            sub_chunk = chunk[j:j + 10]\n            prompt = \"要翻译的部分\\n\".join(f\"{k + 1}|{sub.text}\" for k, sub in enumerate(sub_chunk))\n\n            while True:\n                try:\n                    response = chat.send_message(prompt)\n                    lines = response.text.strip().splitlines()\n                    valid_lines = [line for line in lines if \"|\" in line]\n\n                    if len(valid_lines) == len(sub_chunk):\n                        for k, line in enumerate(valid_lines):\n                            parts = line.split(\"|\", 1)\n                            if len(parts) == 2:\n                                sub_chunk[k].text = parts[1].strip()\n                        translated_subs.extend(sub_chunk)  # 将翻译结果添加到 translated_subs\n\n                        # 提示翻译成功\n                        print(f\"翻译成功: {len(translated_subs)} 行已翻译.\")\n                        time.sleep(5)  # 成功后稍微等待\n                        break\n                    else:\n                        print(\"返回行数与原始字幕不一致，等待重试...\")\n                        time.sleep(5)\n\n                except Exception as e:\n                    print(f\"异常：{e}\")\n                    print(f\"发送：{prompt}\")\n                    print(f\"接收：{response}\")\n                    time.sleep(5)\n\n        # 保存翻译后的字幕，只写入 translated_subs\n        srt_path = os.path.join(config['tsl_path'], filename)\n        translated_subs.save(srt_path, encoding='utf-8')\n        print(f\"字幕已保存到: {srt_path}\")\n\n\n\n\n",
   "metadata": {
    "id": "vP_mIquyLIIT",
    "cellView": "form",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#@title 清理内存\nimport os\nos.kill(os.getpid(), 9)",
   "metadata": {
    "id": "XUCYF8sjWo7z",
    "jupyter": {
     "is_executing": true
    },
    "cellView": "form"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
