{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1挂载云盘"
      ],
      "metadata": {
        "id": "cPPtDIWSblQR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvkI52m5DRsL"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2安装依赖"
      ],
      "metadata": {
        "id": "jus5BKq_bteg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchvision torchaudio -y\n",
        "\n",
        "# Workaround from: https://github.com/m-bain/whisperX/issues/1027#issuecomment-2627525081\n",
        "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# WhisperX-related packages:\n",
        "!pip install ctranslate2==4.4.0\n",
        "!pip install faster-whisper==1.1.0\n",
        "# !pip install git+https://github.com/m-bain/whisperx.git\n",
        "!pip install whisperx==3.3.1\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install libcudnn8=8.9.2.26-1+cuda12.1\n",
        "!apt-get install libcudnn8-dev=8.9.2.26-1+cuda12.1\n",
        "\n",
        "!python -c \"import torch; torch.backends.cuda.matmul.allow_tf32 = True; torch.backends.cudnn.allow_tf32 = True\""
      ],
      "metadata": {
        "id": "IQ1Wqb9HdfkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3导入配置"
      ],
      "metadata": {
        "id": "TTtq2oWrbzaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "path = '/content/gdrive/MyDrive/ASMRASR'\n",
        "with open ('/content/gdrive/MyDrive/ASMRASR/0config.yaml', 'r', encoding='utf-8') as file:\n",
        "  config = yaml.safe_load(file)\n",
        "\n",
        "config[\"pre_path\"] = os.path.join(path, config[\"pre_path\"])\n",
        "config[\"work_path\"] = os.path.join(path, config[\"work_path\"])\n",
        "config[\"asr_path\"] = os.path.join(path, config[\"asr_path\"])\n",
        "config[\"slice_path\"] = os.path.join(path, config[\"slice_path\"])\n",
        "config[\"zh_path\"] = os.path.join(path, config[\"zh_path\"])\n",
        "config[\"result_path\"] = os.path.join(path, config[\"result_path\"])\n",
        "config[\"model_path\"] = os.path.join(path, config[\"model_path\"])"
      ],
      "metadata": {
        "id": "iPEaNVHoSjOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4提取人声"
      ],
      "metadata": {
        "id": "JQaKvXPVb3bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "for root, dirs, files in os.walk(config[\"pre_path\"]):\n",
        "    for filename in files:\n",
        "        if filename.endswith((\".wav\", \".mp3\", \".flac\")):\n",
        "            input_audio = os.path.join(root, filename)\n",
        "            command = [\n",
        "                \"demucs\", \"--two-stems\", \"vocals\",\n",
        "                \"-o\", config[\"work_path\"],\n",
        "                input_audio\n",
        "            ]\n",
        "            subprocess.run(command)\n",
        "            print(f\"提取人声保存至：{input_audio}\")"
      ],
      "metadata": {
        "id": "6gjEKX9iQv1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5转写音频"
      ],
      "metadata": {
        "id": "X69e__kob7aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "\n",
        "audio_paths = []\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "print('设备：', device, '类型：', compute_type)\n",
        "\n",
        "for root, dirs, files in os.walk(config[\"work_path\"]):\n",
        "    for filename in files:\n",
        "        if filename.endswith((\".wav\", \".mp3\")):\n",
        "            audio_path = os.path.join(root, filename)\n",
        "            print(f\"找到: {audio_path}\")\n",
        "            audio_paths.append(audio_path)\n",
        "\n",
        "if audio_paths:\n",
        "    command = [\"whisperx\"] + audio_paths + [\n",
        "        \"--model\", config[\"model\"],\n",
        "        \"--model_cache_only\", str(config[\"model_cache_only\"]),\n",
        "        \"--device_index\", str(config[\"device_index\"]),\n",
        "        \"--device\", device,\n",
        "        \"--batch_size\", str(config[\"batch_size\"]),\n",
        "        \"--compute_type\", compute_type,\n",
        "        \"--output_dir\", config[\"asr_path\"],\n",
        "        \"--output_format\", config[\"output_format\"],\n",
        "        \"--verbose\", str(config[\"verbose\"]),\n",
        "        \"--task\", config[\"task\"],\n",
        "        \"--language\", config[\"language\"],\n",
        "        \"--align_model\", config[\"align_model\"],\n",
        "        \"--interpolate_method\", config[\"interpolate_method\"],\n",
        "        \"--vad_method\", config[\"vad_method\"],\n",
        "        \"--vad_onset\", str(config[\"vad_onset\"]),\n",
        "        \"--vad_offset\", str(config[\"vad_offset\"]),\n",
        "        \"--chunk_size\", str(config[\"chunk_size\"]),\n",
        "        \"--temperature\", str(config[\"temperature\"]),\n",
        "        \"--best_of\", str(config[\"best_of\"]),\n",
        "        \"--beam_size\", str(config[\"beam_size\"]),\n",
        "        \"--patience\", str(config[\"patience\"]),\n",
        "        \"--length_penalty\", str(config[\"length_penalty\"]),\n",
        "        \"--suppress_tokens\", str(config[\"suppress_tokens\"]),\n",
        "        \"--condition_on_previous_text\", str(config[\"condition_on_previous_text\"]),\n",
        "        \"--fp16\", str(config[\"fp16\"]),\n",
        "        \"--temperature_increment_on_fallback\", str(config[\"temperature_increment_on_fallback\"]),\n",
        "        \"--compression_ratio_threshold\", str(config[\"compression_ratio_threshold\"]),\n",
        "        \"--logprob_threshold\", str(config[\"logprob_threshold\"]),\n",
        "        \"--no_speech_threshold\", str(config[\"no_speech_threshold\"]),\n",
        "        \"--highlight_words\", str(config[\"highlight_words\"]),\n",
        "        \"--segment_resolution\", config[\"segment_resolution\"],\n",
        "        \"--threads\", str(config[\"threads\"]),\n",
        "        \"--print_progress\", str(config[\"print_progress\"])\n",
        "    ]\n",
        "\n",
        "    if config.get(\"no_align\"):\n",
        "        command += [\"--no_align\"]\n",
        "    if config.get(\"return_char_alignments\"):\n",
        "        command += [\"--return_char_alignments\"]\n",
        "    if config.get(\"diarize\"):\n",
        "        command += [\"--diarize\"]\n",
        "    if config.get(\"suppress_numerals\"):\n",
        "        command += [\"--suppress_numerals\"]\n",
        "\n",
        "    if config.get(\"min_speakers\"):\n",
        "        command += [\"--min_speakers\", str(config[\"min_speakers\"])]\n",
        "    if config.get(\"max_speakers\"):\n",
        "        command += [\"--max_speakers\", str(config[\"max_speakers\"])]\n",
        "    if config.get(\"initial_prompt\"):\n",
        "        command += [\"--initial_prompt\", config[\"initial_prompt\"]]\n",
        "    if config.get(\"max_line_width\"):\n",
        "        command += [\"--max_line_width\", str(config[\"max_line_width\"])]\n",
        "    if config.get(\"max_line_count\"):\n",
        "        command += [\"--max_line_count\", str(config[\"max_line_count\"])]\n",
        "    if config.get(\"hf_token\"):\n",
        "        command += [\"--hf_token\", config[\"hf_token\"]]\n",
        "\n",
        "    subprocess.run(command)\n"
      ],
      "metadata": {
        "id": "9FLArhie45Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6字幕分句"
      ],
      "metadata": {
        "id": "D5nfQRCfb-94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"ja_ginza_electra\")\n",
        "for root, dirs, files in os.walk(config[\"asr_path\"]):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".json\"):\n",
        "            print(f\"切分中: {filename}\")\n",
        "            json_path = os.path.join(root, filename)  # 获取音频文件的完整路径\n",
        "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            output = {\n",
        "                \"segments\": [],\n",
        "                \"language\": \"ja\"\n",
        "            }\n",
        "\n",
        "            # 遍历每个原始语音段\n",
        "            for segment in data[\"segments\"]:\n",
        "                text = segment[\"text\"]\n",
        "                words = segment[\"words\"]\n",
        "\n",
        "                # 将整段文本进行分句\n",
        "                doc = nlp(text)\n",
        "\n",
        "                # 构造字符位置对应时间戳的索引（按顺序累积）\n",
        "                char_times = []\n",
        "                for word in words:\n",
        "                    char = word[\"word\"]\n",
        "                    start = word.get(\"start\", None)\n",
        "                    end = word.get(\"end\", None)\n",
        "                    char_times.append({\n",
        "                        \"char\": char,\n",
        "                        \"start\": start,\n",
        "                        \"end\": end\n",
        "                    })\n",
        "\n",
        "                current_char_index = 0  # 用于遍历 char_times\n",
        "\n",
        "                # 遍历分好的每个句子\n",
        "                for sent in doc.sents:\n",
        "                    sentence = sent.text.strip()\n",
        "                    if not sentence:\n",
        "                        continue\n",
        "\n",
        "                    sent_chars = list(sentence)\n",
        "                    start_time = None\n",
        "                    end_time = None\n",
        "                    matched_indices = []\n",
        "\n",
        "                    # 逐字符匹配，记录时间戳索引范围\n",
        "                    i = 0\n",
        "                    while i < len(sent_chars) and current_char_index < len(char_times):\n",
        "                        target_char = sent_chars[i]\n",
        "                        current_char = char_times[current_char_index][\"char\"]\n",
        "                        if target_char == current_char:\n",
        "                            matched_indices.append(current_char_index)\n",
        "                            i += 1\n",
        "                        current_char_index += 1\n",
        "\n",
        "                    # 有匹配上的字符，找第一个有时间戳和最后一个有时间戳的\n",
        "                    for idx in matched_indices:\n",
        "                        if start_time is None and char_times[idx][\"start\"] is not None:\n",
        "                            start_time = char_times[idx][\"start\"]\n",
        "                        if char_times[idx][\"end\"] is not None:\n",
        "                            end_time = char_times[idx][\"end\"]\n",
        "\n",
        "                    if start_time is not None and end_time is not None:\n",
        "                        output[\"segments\"].append({\n",
        "                            \"text\": sentence,\n",
        "                            \"start\": start_time,\n",
        "                            \"end\": end_time\n",
        "                        })\n",
        "            print(f\"切分完毕: {filename}\")\n",
        "            basename = os.path.splitext(filename)[0]\n",
        "            output_path = os.path.join(config[\"slice_path\"], f\"{basename}.json\")\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(output, f, ensure_ascii=False, indent=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "UjO972VRNdk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7在线翻译"
      ],
      "metadata": {
        "id": "2EIjnR-scEKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "import google.generativeai\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(config[\"slice_path\"]):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".json\"):\n",
        "            print(f\"正在翻译: {filename}\")\n",
        "            json_path = os.path.join(root, filename)\n",
        "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "            before = [segment[\"text\"] for segment in data[\"segments\"]]\n",
        "            after = []\n",
        "\n",
        "            try:\n",
        "                print(len(before))\n",
        "                google.generativeai.configure(api_key=config[\"genimi_token\"])\n",
        "                model = google.generativeai.GenerativeModel('gemini-2.0-flash')\n",
        "                chat = model.start_chat()\n",
        "                response = chat.send_message(config[\"first_prompt\"])\n",
        "                print(f\"初始回复：\", response.text)\n",
        "\n",
        "                chunk_size = 30\n",
        "                for i in range(0, len(before), chunk_size):\n",
        "                    chunk = before[i:i + chunk_size]\n",
        "\n",
        "                    joined_text = \"\\n\".join(chunk)\n",
        "\n",
        "                    try:\n",
        "                        response = chat.send_message(f'{config[\"common_prompt\"]}{len(chunk)}\\n{joined_text}')\n",
        "                        #print(f\"第{i // chunk_size + 1}组：\", response.text)\n",
        "                        after.extend(response.text.splitlines())\n",
        "\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"第{i // chunk_size + 1}组出错：{e}\")\n",
        "                        after.append('11111111111')\n",
        "                    time.sleep(5)\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"出错了：\", e)\n",
        "            finally:\n",
        "                basename = os.path.splitext(filename)[0]\n",
        "                output_path = os.path.join(root, f\"{basename}.txt\")\n",
        "                with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(\"\\n\".join(after) + \"\\n\")\n",
        "\n",
        "            print(len(after))\n",
        "\n",
        "            for i, segment in enumerate(data[\"segments\"]):\n",
        "                segment[\"text\"] = after[i]\n",
        "            print(f\"翻译完成: {filename}\")\n",
        "            basename = os.path.splitext(filename)[0]\n",
        "            output_path = os.path.join(config[\"zh_path\"], f\"{basename}.json\")\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "_rIchgASUk1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8导出字幕"
      ],
      "metadata": {
        "id": "AWSlYEAxcKqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "from getconfig import get_config\n",
        "\n",
        "\n",
        "def seconds_to_lrc_timestamp(seconds: float) -> str:\n",
        "    minutes = int(seconds // 60)\n",
        "    sec = seconds % 60\n",
        "    return f\"{minutes:02}:{sec:05.2f}\"\n",
        "\n",
        "\n",
        "def seconds_to_srt_timestamp(seconds: float) -> str:\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    secs = int(seconds % 60)\n",
        "    millis = int((seconds - int(seconds)) * 1000)\n",
        "    return f\"{hours:02}:{minutes:02}:{secs:02},{millis:03}\"\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(config[\"zh_path\"]):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".json\"):\n",
        "            json_path = os.path.join(root, filename)\n",
        "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "            segments = data.get(\"segments\", [])\n",
        "            lrc_lines = []\n",
        "            for seg in segments:\n",
        "                start = seconds_to_lrc_timestamp(seg[\"start\"])\n",
        "                end = seconds_to_lrc_timestamp(seg[\"end\"])\n",
        "                text = seg[\"text\"].strip()\n",
        "\n",
        "                lrc_lines.append(f\"[{start}]{text}\")\n",
        "                lrc_lines.append(f\"[{end}]\")\n",
        "\n",
        "            basename = os.path.splitext(filename)[0]\n",
        "            output_path = os.path.join(root, f\"{basename}.lrc\")\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(\"\\n\".join(lrc_lines))\n",
        "\n",
        "            print(f\"已生成 LRC 文件：{output_path}\")\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(config[\"zh_path\"]):\n",
        "    for filename in files:\n",
        "        if (filename.startswith(\"slice-\") or filename.startswith(\"zh-\")) and filename.endswith(\".json\"):\n",
        "            json_path = os.path.join(root, filename)\n",
        "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            segments = data.get(\"segments\", [])\n",
        "\n",
        "            srt_lines = []\n",
        "            for idx, seg in enumerate(segments, 1):\n",
        "                start = seconds_to_srt_timestamp(seg[\"start\"])\n",
        "                end = seconds_to_srt_timestamp(seg[\"end\"])\n",
        "                text = seg[\"text\"].strip()\n",
        "\n",
        "                srt_lines.append(f\"{idx}\")\n",
        "                srt_lines.append(f\"{start} --> {end}\")\n",
        "                srt_lines.append(text)\n",
        "                srt_lines.append(\"\")\n",
        "\n",
        "            basename = os.path.splitext(filename)[0]\n",
        "            output_path = os.path.join(root, f\"{basename}.srt\")\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(\"\\n\".join(srt_lines))\n",
        "\n",
        "            print(f\"已生成 SRT 文件：{output_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6aFDNzWkU3wk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}