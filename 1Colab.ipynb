{
 "cells": [
  {
   "metadata": {
    "id": "L9swbMgOLIIG",
    "cellView": "form"
   },
   "cell_type": "code",
   "source": [
    "#@title 1挂载云盘\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "J62U5RDnLIIM",
    "cellView": "form"
   },
   "cell_type": "code",
   "source": [
    "#@title 2安装依赖\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "# Workaround from: https://github.com/m-bain/whisperX/issues/1027#issuecomment-2627525081\n",
    "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "\n",
    "!pip install ctranslate2==4.4.0\n",
    "!pip install faster-whisper==1.1.1\n",
    "!pip install pysrt\n",
    "!pip install pyannote.audio\n",
    "!pip install imageio-ffmpeg\n",
    "!pip install \"audio-separator[gpu]\"\n",
    "\n",
    "\n",
    "\n",
    "!apt-get update\n",
    "!apt-get install libcudnn8=8.9.2.26-1+cuda12.1\n",
    "!apt-get install libcudnn8-dev=8.9.2.26-1+cuda12.1\n",
    "!pip install pyannote.audio\n",
    "\n",
    "!python -c \"import torch; torch.backends.cuda.matmul.allow_tf32 = True; torch.backends.cudnn.allow_tf32 = True\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "krTHocPaLIIO",
    "cellView": "form"
   },
   "cell_type": "code",
   "source": [
    "#@title 3导入配置\n",
    "import os\n",
    "config = {\n",
    "    \"pre_path\": \"a_pre\",\n",
    "    \"work_path\": \"b_work\",\n",
    "    \"asr_path\": \"c_asr\",\n",
    "    \"tsl_path\": \"d_tsl\",\n",
    "    \"model_path\": \"/content\",\n",
    "    \"log_path\": \"z_log\",\n",
    "    \"overrite_vad\": False,\n",
    "    \"overrite_asr\": True,\n",
    "    \"separator\": \"MDX23C-8KFFT-InstVoc_HQ_2.ckpt\",\n",
    "    \"vad\": \"4evergr8/pyannote-segmentation-3.0\",\n",
    "    \"asr\": \"large-v3-turbo\",\n",
    "    \"translate\": \"gemini-1.5-flash\",\n",
    "    \"batch_size\": 8,\n",
    "    \"min_duration_on\": 0.0,\n",
    "    \"min_duration_off\": 0.2,\n",
    "    \"space\": 3,\n",
    "    \"language\": \"ja\",\n",
    "    \"num_workers\": 15,\n",
    "    \"api_key\": \"\",\n",
    "    \"prompt\": \"\"\"你是一名专业的翻译家，请你按照以下流程进行翻译：\n",
    "第一步：初步直译\n",
    "将日语文本逐行直译成简体中文文本，保留序号、标记符、占位符、换行符等特殊内容，保持原来的格式。\n",
    "\n",
    "第二步：深入校正\n",
    "针对每一句初步译文，可以从语义与语境、专业术语、上下文信息、翻译风格、故事背景、人物设定等等方面出发，进行深入分析和校正。\n",
    "\n",
    "第三步：最终意译与润色\n",
    "整合直译结果和校正建议，进行最终的意译和润色，生成自然流畅、符合简体中文表达习惯的最终译文。\n",
    "\n",
    "### 翻译原则\n",
    "忠实准确地进行翻译。原作是伟大艺术作品，允许使用直白或粗俗的描述，不应随意删减或篡改，不应回避或淡化，以保持作品的原貌。\n",
    "只输出最终的翻译结果，不输出中间结果，结合上下文进行翻译，注意用“|”分割的序号，我会先将原文发给你用来熟悉上下文，然后分批次获取翻译内容，原文如下：\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_path(config_value):\n",
    "    path = os.path.join('/content/gdrive/MyDrive/ASMRASR', config_value)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(f\"创建文件夹: {path}\")\n",
    "    return path\n",
    "\n",
    "config[\"pre_path\"] = get_path(config[\"pre_path\"])\n",
    "config[\"work_path\"] = get_path(config[\"work_path\"])\n",
    "config[\"asr_path\"] = get_path(config[\"asr_path\"])\n",
    "config[\"tsl_path\"] = get_path(config[\"tsl_path\"])\n",
    "config[\"log_path\"] = get_path(config[\"log_path\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "PYephQIxLIIQ",
    "cellView": "form"
   },
   "cell_type": "code",
   "source": [
    "#@title 4人声分离\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from imageio_ffmpeg import get_ffmpeg_exe\n",
    "from audio_separator.separator import Separator\n",
    "\n",
    "ffmpeg = get_ffmpeg_exe()\n",
    "os.system(\"chcp 65001\")\n",
    "\n",
    "\n",
    "for audio_filename in os.listdir(config[\"pre_path\"]):\n",
    "    if audio_filename.endswith((\".wav\", \".mp3\", \".flac\")):\n",
    "        basename = os.path.splitext(audio_filename)[0]\n",
    "        slice_path = os.path.join(config[\"pre_path\"], f\"{basename}-slice\")\n",
    "        split_path = os.path.join(config[\"pre_path\"], f\"{basename}-split\")\n",
    "        audio_path = os.path.join(config[\"pre_path\"], audio_filename)\n",
    "        if os.path.isfile(audio_path):\n",
    "\n",
    "            if os.path.exists(slice_path):\n",
    "                shutil.rmtree(slice_path)  # 删除原文件夹及其内容\n",
    "            os.makedirs(slice_path)  # 创建新文件夹\n",
    "\n",
    "            segment_length = 1200  # 20 分钟 = 1200 秒\n",
    "            command = [\n",
    "                ffmpeg, \"-i\", audio_path,  # 输入音频文件\n",
    "                \"-f\", \"segment\",  # 使用 segment 格式进行切割\n",
    "                \"-segment_time\", str(segment_length),  # 设置每段的时长（单位：秒）\n",
    "                \"-c\", \"copy\",  # 保持原始编码（无损切割）\n",
    "                os.path.join(slice_path, \"%03d.wav\")  # 输出文件的命名格式\n",
    "            ]\n",
    "            subprocess.run(command)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            separator = Separator(\n",
    "                model_file_dir=config[\"model_path\"],\n",
    "                output_dir=split_path,\n",
    "                output_single_stem=\"vocals\",\n",
    "                sample_rate=16000,\n",
    "                mdxc_params={\"segment_size\": 256, \"override_model_segment_size\": False, \"batch_size\": config[\"batch_size\"],\n",
    "                             \"overlap\": 8, \"pitch_shift\": 0}\n",
    "            )\n",
    "            separator.load_model(model_filename=config[\"separator\"])\n",
    "            for filename in os.listdir(slice_path):\n",
    "                if filename.endswith(\".wav\"):\n",
    "                    slice_basename = os.path.splitext(filename)[0]  # 比如 '001'\n",
    "                    exists = any(name.startswith(slice_basename) for name in os.listdir(split_path))\n",
    "                    if not exists:\n",
    "                        output_files = separator.separate(os.path.join(slice_path, filename))\n",
    "                        print(f\"<UNK>{len(output_files)}\")\n",
    "                    else:\n",
    "                        print(f\"已存在分离结果，跳过：{filename}\")\n",
    "\n",
    "            file_list = sorted(\n",
    "                [f for f in os.listdir(split_path) if f.endswith(\".wav\")],\n",
    "                key=lambda x: int(x[:3])\n",
    "            )\n",
    "            with open(os.path.join(config[\"pre_path\"], f\"{basename}_list.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                for f_name in file_list:\n",
    "                    full_path = os.path.join(split_path, f_name)\n",
    "                    f.write(f\"file '{full_path}'\\n\")\n",
    "\n",
    "            basename = os.path.splitext(audio_filename)[0]\n",
    "            output_path = os.path.join(config[\"work_path\"], f\"{basename}.wav\")\n",
    "            command = [\n",
    "                ffmpeg,\n",
    "                \"-f\", \"concat\",\n",
    "                \"-safe\", \"0\",\n",
    "                \"-i\", os.path.join(config[\"pre_path\"], f\"{basename}_list.txt\"),\n",
    "                \"-c\", \"copy\",\n",
    "                output_path\n",
    "            ]\n",
    "            subprocess.run(command)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "0t_AN__qLIIS",
    "cellView": "form"
   },
   "cell_type": "code",
   "source": [
    "#@title 5音频转写\n",
    "import gc\n",
    "import pysrt\n",
    "import torch\n",
    "import numpy as np\n",
    "from pyannote.audio import Model\n",
    "from faster_whisper import WhisperModel\n",
    "import librosa\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "import os\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
    "print('设备:', device, '类型:', compute_type)\n",
    "\n",
    "\n",
    "for filename in os.listdir(config[\"work_path\"]):\n",
    "    if not filename.endswith((\".wav\", \".mp3\", \".flac\")):\n",
    "        continue\n",
    "    audio_path = os.path.join(config[\"work_path\"], filename)\n",
    "    print(f\"\\n处理音频: {audio_path}\")\n",
    "    basename = os.path.splitext(filename)[0]\n",
    "\n",
    "\n",
    "    vad_log_path = os.path.join(config[\"log_path\"], f\"vad-{basename}.srt\")\n",
    "    if not os.path.exists(vad_log_path) or config[\"overwrite_vad\"]:\n",
    "        vad_model = Model.from_pretrained(checkpoint=config[\"vad\"], cache_dir=config[\"model_path\"])\n",
    "        vad_model.to(torch.device(device))\n",
    "        vad_pipeline = VoiceActivityDetection(segmentation=vad_model)\n",
    "        vad_pipeline.instantiate({\n",
    "            \"min_duration_on\": config[\"min_duration_on\"],\n",
    "            \"min_duration_off\": config[\"min_duration_off\"],\n",
    "        })\n",
    "\n",
    "\n",
    "        vad_result = vad_pipeline(str(audio_path))\n",
    "        del vad_pipeline, vad_model\n",
    "\n",
    "        timeline = vad_result.get_timeline()\n",
    "        vad_log = pysrt.SubRipFile()\n",
    "\n",
    "\n",
    "        group_duration = 1800  # 每组时长：30分钟\n",
    "        for segment in timeline:\n",
    "            group_index = int(segment.end // group_duration)\n",
    "            group_base_idx = 1000 + group_index * 1000\n",
    "            sub_index = group_base_idx + len(\n",
    "                [s for s in vad_log if group_base_idx <= s.index < group_base_idx + 1000])\n",
    "\n",
    "            sub = pysrt.SubRipItem(\n",
    "                index=sub_index,\n",
    "                start=pysrt.SubRipTime.from_ordinal(int(segment.start * 1000)),\n",
    "                end=pysrt.SubRipTime.from_ordinal(int(segment.end * 1000)),\n",
    "                text=\"默认占位\"+str(sub_index)\n",
    "            )\n",
    "            vad_log.append(sub)\n",
    "\n",
    "        vad_log.save(vad_log_path)\n",
    "        print(f\"VAD记录写入: {vad_log_path}\")\n",
    "    else:\n",
    "        vad_log = pysrt.open(vad_log_path)\n",
    "        print('VAD记录存在，跳过')\n",
    "\n",
    "\n",
    "\n",
    "    slice_log = pysrt.SubRipFile()  # 用于存储调整后的字幕\n",
    "    silence_duration = config[\"space\"]  # 获取配置中的静音时间\n",
    "    current_group_end = 0.0  # 当前组的结束时间\n",
    "\n",
    "    for subtitle in vad_log:\n",
    "        segment_start = subtitle.start.ordinal / 1000  # 转换为秒\n",
    "        segment_end = subtitle.end.ordinal / 1000  # 转换为秒\n",
    "\n",
    "        # 如果是该组的第一个段（序号能被1000整除）\n",
    "        if subtitle.index % 1000 == 0:\n",
    "            group_start = 0.0  # 设置为0，确保每组的第一个段从00:00:00开始\n",
    "        else:\n",
    "            group_start = current_group_end  # 否则按照上一段的结束时间进行处理\n",
    "\n",
    "        group_end = group_start + (segment_end - segment_start)  # 保持时间段的长度不变\n",
    "\n",
    "        # 更新字幕的开始和结束时间戳\n",
    "        subtitle.start = pysrt.SubRipTime.from_ordinal(int(group_start * 1000))  # 转换为毫秒并设置新的开始时间\n",
    "        subtitle.end = pysrt.SubRipTime.from_ordinal(int(group_end * 1000))  # 设置新的结束时间\n",
    "\n",
    "        # 更新当前组的结束时间\n",
    "        current_group_end = group_end + silence_duration  # 下一组的开始时间是当前组的结束时间+静音间隔\n",
    "\n",
    "        # 将处理后的字幕项添加到slice_log中\n",
    "        slice_log.append(subtitle)\n",
    "\n",
    "    slice_log_path = os.path.join(config[\"log_path\"], f\"slice-{basename}.srt\")\n",
    "    slice_log.save(slice_log_path)\n",
    "    print(f\"slice记录写入: {slice_log_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    asr_log_path = os.path.join(config[\"log_path\"], f\"asr-{basename}.srt\")\n",
    "    if not os.path.exists(asr_log_path) or config[\"overwrite_asr\"]:\n",
    "        vad_log = pysrt.open(vad_log_path)\n",
    "        audio, sr = librosa.load(str(audio_path), sr=16000, mono=True)\n",
    "\n",
    "        audios = []  # 每个元素是一个 numpy 音频数组\n",
    "        silence_duration = config[\"space\"]  # 静音间隔\n",
    "        silence = np.zeros(int(sr * silence_duration), dtype=np.float32)  # 静音段\n",
    "\n",
    "        group_dict = {}  # 存储各组的音频段，key 是组号（如 1, 2, 3）\n",
    "\n",
    "        # 处理字幕，根据字幕 index 决定所属组\n",
    "        for subtitle in vad_log:\n",
    "            segment_start = subtitle.start.ordinal / 1000  # 秒\n",
    "            segment_end = subtitle.end.ordinal / 1000  # 秒\n",
    "            index = subtitle.index\n",
    "            group_id = index // 1000  # 1000~1999 为第 1 组，2000~2999 为第 2 组，以此类推\n",
    "\n",
    "            start_sample = int(segment_start * sr)\n",
    "            end_sample = int(segment_end * sr)\n",
    "            audio_seg = audio[start_sample:end_sample]\n",
    "\n",
    "            if group_id not in group_dict:\n",
    "                group_dict[group_id] = []\n",
    "            group_dict[group_id].append(audio_seg)\n",
    "\n",
    "        # 拼接每组音频，并插入静音段\n",
    "        for group_id in sorted(group_dict):\n",
    "            group_audio = []\n",
    "            for i, segment in enumerate(group_dict[group_id]):\n",
    "                if i > 0:\n",
    "                    group_audio.append(silence)\n",
    "                group_audio.append(segment)\n",
    "\n",
    "            group_array = np.concatenate(group_audio) if group_audio else np.array([], dtype=np.float32)\n",
    "            audios.append(group_array)\n",
    "\n",
    "        print(f\"音频分组完成，共 {len(audios)} 组。\")\n",
    "        del audio\n",
    "\n",
    "\n",
    "        asr_model = WhisperModel(\n",
    "            model_size_or_path=config[\"asr\"],\n",
    "            device=device,\n",
    "            compute_type=compute_type,\n",
    "            download_root=config[\"model_path\"],\n",
    "            num_workers=config[\"num_workers\"]\n",
    "\n",
    "        )\n",
    "\n",
    "        asr_log = pysrt.SubRipFile()\n",
    "        base_index = 1000  # 初始组编号起点\n",
    "\n",
    "        for group_idx, audio in enumerate(audios, start=1):\n",
    "            start_index = group_idx * base_index\n",
    "\n",
    "            segments, _ = asr_model.transcribe(\n",
    "                audio=audio,\n",
    "                language=config['language'],\n",
    "                task=\"transcribe\",\n",
    "                log_progress=True,\n",
    "                beam_size=5,\n",
    "                best_of=5,\n",
    "                patience=1,\n",
    "                length_penalty=1,\n",
    "                repetition_penalty=1.1,  # 稍微提高抑制重复的倾向\n",
    "                no_repeat_ngram_size=3,  # 阻止 ngram 重复\n",
    "                temperature=[0.2, 0.4, 0.6, 0.8, 1.0],  # 稍高起步避免死循环\n",
    "                compression_ratio_threshold=2.4,\n",
    "                log_prob_threshold=-1.0,\n",
    "                no_speech_threshold=0.6,\n",
    "                condition_on_previous_text=True,  # 保留上下文\n",
    "                prompt_reset_on_temperature=0.5,\n",
    "                prefix=None,\n",
    "                suppress_blank=True,\n",
    "                suppress_tokens=[-1],\n",
    "                without_timestamps=False,\n",
    "                max_initial_timestamp=1.0,\n",
    "                word_timestamps=True,\n",
    "                hallucination_silence_threshold=1.5,\n",
    "                prepend_punctuations=\"\\\"'“¿([{-\",\n",
    "                append_punctuations=\"\\\"'.。,，!！?？:：”)]}、\",\n",
    "                multilingual=False,\n",
    "                vad_filter=False,\n",
    "                clip_timestamps=\"0\",\n",
    "                language_detection_threshold=None,\n",
    "                language_detection_segments=1,\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for i, seg in enumerate(segments):\n",
    "                seg_start = seg.start\n",
    "                seg_end = seg.end\n",
    "                seg_text = seg.text.strip()\n",
    "\n",
    "                subtitle = pysrt.SubRipItem(\n",
    "                    index=start_index + i,\n",
    "                    start=pysrt.SubRipTime.from_ordinal(int(seg_start * 1000)),\n",
    "                    end=pysrt.SubRipTime.from_ordinal(int(seg_end * 1000)),\n",
    "                    text=seg_text\n",
    "                )\n",
    "                asr_log.append(subtitle)\n",
    "\n",
    "        # 保存识别结果\n",
    "\n",
    "            asr_log.save(asr_log_path)\n",
    "            print(f\"ASR记录写入: {asr_log_path}\")\n",
    "    else:\n",
    "        asr_log = pysrt.open(asr_log_path)\n",
    "        print('ASR记录存在，跳过')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for slice_sub in slice_log:\n",
    "        segment_start = slice_sub.start.ordinal / 1000\n",
    "        segment_end = slice_sub.end.ordinal / 1000\n",
    "        segment_index = slice_sub.index\n",
    "        group_prefix = (segment_index // 1000) * 1000\n",
    "\n",
    "        max_overlap = 0.0\n",
    "        best_match = None\n",
    "\n",
    "        # 查找当前组中所有 asr_log 字幕（同一千段内）\n",
    "        for asr_sub in asr_log:\n",
    "            if (asr_sub.index // 1000) * 1000 != group_prefix:\n",
    "                continue\n",
    "\n",
    "            seg_start = asr_sub.start.ordinal / 1000\n",
    "            seg_end = asr_sub.end.ordinal / 1000\n",
    "\n",
    "            # 重合检测\n",
    "            overlap_start = max(seg_start, segment_start)\n",
    "            overlap_end = min(seg_end, segment_end)\n",
    "            overlap_duration = overlap_end - overlap_start\n",
    "\n",
    "            if overlap_duration > max_overlap:\n",
    "                max_overlap = overlap_duration\n",
    "                best_match = asr_sub\n",
    "\n",
    "        if best_match is not None:\n",
    "            slice_sub.text = best_match.text\n",
    "\n",
    "    # 保存更新后的 slice_log\n",
    "    match_path = os.path.join(config[\"log_path\"], f\"match-{basename}.srt\")\n",
    "    slice_log.save(match_path)\n",
    "    print(f\"match结果写入: {match_path}\")\n",
    "\n",
    "    vad_log = pysrt.open(vad_log_path)\n",
    "\n",
    "    for vad_sub in vad_log:\n",
    "        for slice_sub in slice_log:\n",
    "            idx = slice_sub.index\n",
    "\n",
    "            if vad_sub.index == idx:\n",
    "                vad_sub.text = slice_sub.text\n",
    "                break  # 找到对应的字幕后可以停止循环，避免多次匹配\n",
    "\n",
    "    for i, sub in enumerate(vad_log, 1):\n",
    "        sub.index = i\n",
    "\n",
    "    result_path = os.path.join(config[\"asr_path\"], f\"{basename}.srt\")\n",
    "    vad_log.save(result_path)\n",
    "    print(f\"结果写入: {result_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "vP_mIquyLIIT",
    "cellView": "form"
   },
   "cell_type": "code",
   "source": [
    "#@title 6结果翻译\n",
    "import os\n",
    "import time\n",
    "import pysrt\n",
    "from google import genai\n",
    "from getconfig import get_config\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "\n",
    "for filename in os.listdir(config[\"asr_path\"]):\n",
    "    if not filename.endswith(\".srt\"):\n",
    "        continue\n",
    "    basename = os.path.splitext(filename)[0]\n",
    "    src_path = os.path.join(config[\"asr_path\"], filename)\n",
    "    dst_path = os.path.join(config[\"tsl_path\"], filename)\n",
    "\n",
    "    original_subs = pysrt.open(src_path, encoding='utf-8')\n",
    "\n",
    "    # 创建新的 SubRipFile 并重新编号\n",
    "    original_subs = pysrt.SubRipFile(items=[sub for sub in original_subs if \"默认占位\" not in sub.text])\n",
    "\n",
    "\n",
    "    original_subs.clean_indexes()\n",
    "\n",
    "\n",
    "    try:\n",
    "        translated_subs = pysrt.open(dst_path, encoding='utf-8')\n",
    "    except:\n",
    "        translated_subs = pysrt.SubRipFile()\n",
    "\n",
    "    # 如果翻译的字幕与原字幕一样长，跳过\n",
    "    if len(translated_subs) == len(original_subs):\n",
    "        print(f\"文件 {filename} 已经全部翻译完，跳过.\")\n",
    "        continue\n",
    "\n",
    "    # 计算剩余未翻译的部分\n",
    "    original_subs = original_subs[len(translated_subs):]\n",
    "\n",
    "    # 翻译剩余部分\n",
    "    for i in range(0, len(original_subs), 200):\n",
    "        chunk = original_subs[i:i + 200]\n",
    "\n",
    "        client = genai.Client(api_key=config[\"api_key\"])\n",
    "        chat = client.chats.create(\n",
    "            model=config['translate'],\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=config[\"prompt\"] + \"\\n\" + \"\\n\".join(\n",
    "                    sub.text for sub in chunk),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for j in range(0, len(chunk), 10):\n",
    "            sub_chunk = chunk[j:j + 10]\n",
    "            prompt = \"要翻译的部分\\n\".join(f\"{k + 1}|{sub.text}\" for k, sub in enumerate(sub_chunk))\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    response = chat.send_message(prompt)\n",
    "                    lines = response.text.strip().splitlines()\n",
    "                    valid_lines = [line for line in lines if \"|\" in line]\n",
    "\n",
    "                    if len(valid_lines) == len(sub_chunk):\n",
    "                        for k, line in enumerate(valid_lines):\n",
    "                            parts = line.split(\"|\", 1)\n",
    "                            if len(parts) == 2:\n",
    "                                sub_chunk[k].text = parts[1].strip()\n",
    "                        translated_subs.extend(sub_chunk)  # 将翻译结果添加到 translated_subs\n",
    "\n",
    "                        # 提示翻译成功\n",
    "                        print(f\"翻译成功: {len(translated_subs)} 行已翻译.\")\n",
    "                        time.sleep(5)  # 成功后稍微等待\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"返回行数与原始字幕不一致，等待重试...\")\n",
    "                        time.sleep(5)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"异常：{e}\")\n",
    "                    print(f\"发送：{prompt}\")\n",
    "                    print(f\"接收：{response}\")\n",
    "                    time.sleep(5)\n",
    "\n",
    "        # 保存翻译后的字幕，只写入 translated_subs\n",
    "        srt_path = os.path.join(config['tsl_path'], filename)\n",
    "        translated_subs.save(srt_path, encoding='utf-8')\n",
    "        print(f\"字幕已保存到: {srt_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XUCYF8sjWo7z",
    "jupyter": {
     "is_executing": true
    },
    "cellView": "form"
   },
   "source": [
    "#@title 清理内存\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
