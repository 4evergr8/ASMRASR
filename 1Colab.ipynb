{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title 1安装依赖\n",
        "!pip uninstall torch torchvision torchaudio -y\n",
        "\n",
        "# Workaround from: https://github.com/m-bain/whisperX/issues/1027#issuecomment-2627525081\n",
        "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "\n",
        "!pip install ctranslate2==4.4.0\n",
        "!pip install faster-whisper==1.1.1\n",
        "!pip install pysrt\n",
        "!pip install pyannote.audio\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install \"audio-separator[gpu]\"\n",
        "\n",
        "\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install libcudnn8=8.9.2.26-1+cuda12.1\n",
        "!apt-get install libcudnn8-dev=8.9.2.26-1+cuda12.1\n",
        "!pip install pyannote.audio\n",
        "\n",
        "!python -c \"import torch; torch.backends.cuda.matmul.allow_tf32 = True; torch.backends.cudnn.allow_tf32 = True\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "rAR1r6_t2BgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvkI52m5DRsL",
        "outputId": "971dc5da-e32b-4fd7-9ee5-f0041c6710a3",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#@title 2挂载云盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB1nRNuqnu9F",
        "outputId": "1ab4c8d5-9605-4f32-e9e7-5a22f818e192",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "创建文件夹: /content/gdrive/MyDrive/ASMRASR/a_pre\n",
            "创建文件夹: /content/gdrive/MyDrive/ASMRASR/b_work\n",
            "创建文件夹: /content/gdrive/MyDrive/ASMRASR/c_asr\n",
            "创建文件夹: /content/gdrive/MyDrive/ASMRASR/z_log\n"
          ]
        }
      ],
      "source": [
        "#@title 3导入配置\n",
        "import os\n",
        "config = {\n",
        "    \"pre_path\": \"a_pre\",  # 存放待提取视频\n",
        "    \"work_path\": \"b_work\",  # 存放待分离音频\n",
        "    \"asr_path\": \"c_asr\",  # 存放转写结果\n",
        "    \"tsl_path\": \"d_tsl\",\n",
        "    \"model_path\": \"/content\",  # 存放模型\n",
        "    \"log_path\": \"z_log\",  # 存放记录\n",
        "\n",
        "    \"prompt\": \"\",  # 被识别音频的标题，作为额外信息输入\n",
        "    \"language\": \"ja\",\n",
        "    \"space\": 2,  # 人声段切割后间隔，单位为秒\n",
        "    \"min_duration_on\": 0.0,  # 一段语音至少要持续这么久，才认为是“有效语音段”，优先级高于min_duration_off\n",
        "    \"min_duration_off\": 0.2,  # 两段语音之间的静音至少要持续这么久，才认为是“真正的停顿”\n",
        "\n",
        "    \"separator\": \"htdemucs_ft.yaml\",\n",
        "    \"vad\": \"4evergr8/pyannote-segmentation-3.0\",  # VAD模型，来自Pyannote，“修复了原版模型强制登陆的bug”\n",
        "    \"asr\": \"zh-plus/faster-whisper-large-v2-japanese-5k-steps\",  # Whisper模型，仅支持ctranslate2格式\n",
        "\n",
        "    \"output\": [  # 输出文件格式，可多选\n",
        "        \"lrc\",\n",
        "        \"log\",\n",
        "        # \"srt\",\n",
        "        # \"vtt\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "def get_path(config_value):\n",
        "    path = os.path.join('/content/gdrive/MyDrive/ASMRASR', config_value)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"创建文件夹: {path}\")\n",
        "    return path\n",
        "\n",
        "config[\"pre_path\"] = get_path(config[\"pre_path\"])\n",
        "config[\"work_path\"] = get_path(config[\"work_path\"])\n",
        "config[\"asr_path\"] = get_path(config[\"asr_path\"])\n",
        "config[\"log_path\"] = get_path(config[\"log_path\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4人声分离\n",
        "from audio_separator.separator import Separator\n",
        "separator = Separator(\n",
        "            output_dir=config[\"work_path\"],\n",
        "            model_file_dir=config[\"model_path\"],\n",
        "            output_single_stem=\"vocals\",\n",
        "            demucs_params={\"segment_size\": \"22\", \"shifts\": 2, \"overlap\": 0.25, \"segments_enabled\": True},\n",
        "\n",
        "        )\n",
        "separator.load_model(model_filename=config[\"separator\"])\n",
        "output_files = separator.separate(config[\"pre_path\"])\n",
        "print(f\"<UNK>{len(output_files)}\")"
      ],
      "metadata": {
        "id": "SjEHA3LPVonx",
        "outputId": "ea7fa906-e4f3-4f24-a23e-4c6d911a85ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:audio_separator.separator.separator:Separator version 0.32.0 instantiating with output_dir: /content/gdrive/MyDrive/ASMRASR/b_work, output_format: WAV\n",
            "INFO:audio_separator.separator.separator:Using model directory from model_file_dir parameter: /content\n",
            "INFO:audio_separator.separator.separator:Operating System: Linux #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025\n",
            "INFO:audio_separator.separator.separator:System: Linux Node: d08638d6f997 Release: 6.1.123+ Machine: x86_64 Proc: x86_64\n",
            "INFO:audio_separator.separator.separator:Python Version: 3.11.12\n",
            "INFO:audio_separator.separator.separator:PyTorch Version: 2.5.1+cu121\n",
            "INFO:audio_separator.separator.separator:FFmpeg installed: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "INFO:audio_separator.separator.separator:ONNX Runtime GPU package installed with version: 1.21.1\n",
            "INFO:audio_separator.separator.separator:ONNX Runtime CPU package installed with version: 1.21.1\n",
            "INFO:audio_separator.separator.separator:CUDA is available in Torch, setting Torch device to CUDA\n",
            "INFO:audio_separator.separator.separator:ONNXruntime has CUDAExecutionProvider available, enabling acceleration\n",
            "INFO:audio_separator.separator.separator:Loading model htdemucs_ft.yaml...\n",
            "INFO:audio_separator.separator.separator:Demucs Separator initialisation complete\n",
            "INFO:audio_separator.separator.separator:Load model duration: 00:00:01\n",
            "INFO:audio_separator.separator.separator:Processing file: /content/gdrive/MyDrive/ASMRASR/a_pre/MIDV-771.wav\n",
            "INFO:audio_separator.separator.separator:Starting separation process for audio_file_path: /content/gdrive/MyDrive/ASMRASR/a_pre/MIDV-771.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FLArhie45Cu",
        "outputId": "dd7c60a4-2c29-45b6-f308-b1c83789543b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "设备: cuda 类型: float16\n",
            "\n",
            "处理音频: /content/gdrive/MyDrive/ASMRASR/b_work/ゆるギャルJK!!_track02.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
            "It can be re-enabled by calling\n",
            "   >>> import torch\n",
            "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
            "   >>> torch.backends.cudnn.allow_tf32 = True\n",
            "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log写入: /content/gdrive/MyDrive/ASMRASR/z_log/before-ゆるギャルJK!!_track02.srt\n",
            "字幕写入: /content/gdrive/MyDrive/ASMRASR/c_asr/ゆるギャルJK!!_track02.srt\n"
          ]
        }
      ],
      "source": [
        "#@title 5音频转写\n",
        "import gc\n",
        "import pysrt\n",
        "import torch\n",
        "import numpy as np\n",
        "from pyannote.audio import Model\n",
        "from faster_whisper import WhisperModel\n",
        "import librosa\n",
        "from pyannote.audio.pipelines import VoiceActivityDetection\n",
        "from dataclasses import dataclass\n",
        "import os\n",
        "\n",
        "\n",
        "# 数据结构\n",
        "@dataclass\n",
        "class AudioSegmentInfo:\n",
        "    start: float\n",
        "    end: float\n",
        "    group_start: float\n",
        "    group_end: float\n",
        "    text: str = \"...\"\n",
        "\n",
        "@dataclass\n",
        "class AudioData:\n",
        "    audio_array: np.ndarray\n",
        "    segment_info_list: list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 硬件\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "print('设备:', device, '类型:', compute_type)\n",
        "\n",
        "# 只初始化一次ASR模型（不会在每个音频内循环初始化）\n",
        "\n",
        "\n",
        "# 遍历所有音频\n",
        "for root, dirs, files in os.walk(config[\"work_path\"]):\n",
        "    for filename in files:\n",
        "        audio_path = os.path.join(root, filename)\n",
        "        basename = os.path.splitext(filename)[0]\n",
        "        print(f\"\\n处理音频: {audio_path}\")\n",
        "\n",
        "        # Step 1: 加载音频\n",
        "        audio, sr = librosa.load(str(audio_path), sr=16000, mono=True)\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        vad_model = Model.from_pretrained(checkpoint=config[\"vad\"], cache_dir=config[\"model_path\"])\n",
        "        vad_model.to(torch.device(device))\n",
        "        vad_pipeline = VoiceActivityDetection(segmentation=vad_model)\n",
        "        vad_pipeline.instantiate({\n",
        "            \"min_duration_on\": config[\"min_duration_on\"],\n",
        "            \"min_duration_off\": config[\"min_duration_off\"],\n",
        "        })\n",
        "\n",
        "        vad_result = vad_pipeline(str(audio_path))\n",
        "\n",
        "        # VAD结束，释放模型和缓存\n",
        "        del vad_pipeline, vad_model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Step 3: 切人声片段\n",
        "        timeline = vad_result.get_timeline()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        audio_groups = []\n",
        "        group_start_limit = 30 * 60  # 每组音频的时间限制，30分钟\n",
        "\n",
        "        silence = np.zeros(int(sr * config[\"space\"]), dtype=audio.dtype)\n",
        "\n",
        "        audio_groups = []\n",
        "        group_start_limit = 30 * 60  # 每组音频的时间限制，30分钟\n",
        "        silence_duration = config[\"space\"]\n",
        "        silence = np.zeros(int(sr * silence_duration), dtype=audio.dtype)\n",
        "\n",
        "        for segment in timeline:\n",
        "            # 计算当前 segment 的时间\n",
        "            segment_start = segment.start\n",
        "            segment_end = segment.end\n",
        "\n",
        "            # 所属分组\n",
        "            group_index = int(segment_end // group_start_limit)\n",
        "\n",
        "            # 创建新分组（如果尚不存在）\n",
        "            while len(audio_groups) <= group_index:\n",
        "                audio_groups.append(AudioData(audio_array=np.array([]), segment_info_list=[]))\n",
        "\n",
        "            # 添加 segment_info 到对应组\n",
        "            audio_groups[group_index].segment_info_list.append(\n",
        "                AudioSegmentInfo(start=segment_start, end=segment_end, group_start=0.0, group_end=0.0)\n",
        "            )\n",
        "\n",
        "        # 对每组音频进行拼接处理\n",
        "        for audio_group in audio_groups:\n",
        "            group_audio = []\n",
        "            current_group_end = 0.0\n",
        "\n",
        "            for i, segment in enumerate(audio_group.segment_info_list):\n",
        "                segment_start = segment.start\n",
        "                segment_end = segment.end\n",
        "\n",
        "                # 计算拼接后的位置\n",
        "                group_start = current_group_end\n",
        "                group_end = group_start + (segment_end - segment_start)\n",
        "\n",
        "                # 更新 group_start 和 group_end\n",
        "                segment.group_start = group_start\n",
        "                segment.group_end = group_end\n",
        "\n",
        "                # 提取音频段\n",
        "                start_sample = int(segment_start * sr)\n",
        "                end_sample = int(segment_end * sr)\n",
        "                audio_seg = audio[start_sample:end_sample]\n",
        "\n",
        "                # 加入静音（除首段）\n",
        "                if i > 0:\n",
        "                    group_audio.append(silence)\n",
        "                group_audio.append(audio_seg)\n",
        "\n",
        "                # 更新下一段的起点\n",
        "                current_group_end = group_end + (\n",
        "                    silence_duration if i < len(audio_group.segment_info_list) - 1 else 0)\n",
        "\n",
        "            if group_audio:\n",
        "                audio_group.audio_array = np.concatenate(group_audio)\n",
        "            else:\n",
        "                audio_group.audio_array = np.array([])\n",
        "\n",
        "        del audio\n",
        "        subs = pysrt.SubRipFile()\n",
        "        for audio_group in audio_groups:\n",
        "            for segment in audio_group.segment_info_list:\n",
        "                sub = pysrt.SubRipItem(\n",
        "                    index=len(subs) + 1,  # 字幕索引\n",
        "                    start=pysrt.SubRipTime.from_ordinal(int(segment.group_start * 1000)),  # 转换 start 为 SRT 时间格式\n",
        "                    end=pysrt.SubRipTime.from_ordinal(int(segment.group_end * 1000)),  # 转换 end 为 SRT 时间格式\n",
        "                    text=segment.text  # 字幕内容\n",
        "                )\n",
        "                subs.append(sub)\n",
        "\n",
        "\n",
        "        srt_path = os.path.join(config[\"log_path\"], f\"before-{basename}.srt\")\n",
        "        subs.save(srt_path)\n",
        "        print(f\"log写入: {srt_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        asr_model = WhisperModel(\n",
        "            config[\"asr\"],\n",
        "            device=device,\n",
        "            compute_type=compute_type,\n",
        "            download_root=config[\"model_path\"],\n",
        "            num_workers=20\n",
        "        )\n",
        "        subs = pysrt.SubRipFile()\n",
        "        for audio_group in audio_groups:\n",
        "            segments, _ = asr_model.transcribe(\n",
        "                audio=audio_group.audio_array,\n",
        "                beam_size=2,\n",
        "                vad_filter=False,\n",
        "                initial_prompt=basename,\n",
        "                language=config['language']\n",
        "            )\n",
        "\n",
        "            for seg in segments:\n",
        "                seg_start = seg.start\n",
        "                seg_end = seg.end\n",
        "                seg_text = seg.text.strip()\n",
        "\n",
        "                best_match = None\n",
        "                max_overlap = 0.0\n",
        "\n",
        "                subtitle = pysrt.SubRipItem(\n",
        "                    index=len(subs) + 1,\n",
        "                    start=pysrt.SubRipTime.from_ordinal(int(seg_start * 1000)),  # 转换为毫秒\n",
        "                    end=pysrt.SubRipTime.from_ordinal(int(seg_end * 1000)),  # 转换为毫秒\n",
        "                    text=seg_text\n",
        "                )\n",
        "                subs.append(subtitle)\n",
        "\n",
        "                for segment_info in audio_group.segment_info_list:\n",
        "                    # 求开始时间的最大值和结束时间的最小值\n",
        "                    overlap_start = max(seg_start, segment_info.group_start)\n",
        "                    overlap_end = min(seg_end, segment_info.group_end)\n",
        "\n",
        "                    # 如果重合时间大于零，计算重合时长\n",
        "                    overlap_duration = max(0.0, overlap_end - overlap_start)\n",
        "\n",
        "                    # 只有当重合时长大于零时，才可能是一个有效的匹配\n",
        "                    if overlap_duration >= max_overlap:\n",
        "                        max_overlap = overlap_duration\n",
        "                        best_match = segment_info\n",
        "\n",
        "                if best_match and max_overlap > 0:\n",
        "                    best_match.text = seg_text\n",
        "\n",
        "        srt_path = os.path.join(config[\"log_path\"], f\"asr-{basename}.srt\")\n",
        "        subs.save(srt_path)\n",
        "\n",
        "\n",
        "        del asr_model\n",
        "        gc.collect()\n",
        "        subs = pysrt.SubRipFile()\n",
        "\n",
        "        for audio_group in audio_groups:\n",
        "            # 创建 SRT 字幕文件对象\n",
        "\n",
        "\n",
        "            for segment in audio_group.segment_info_list:\n",
        "                # 将每个 segment 信息转换为 SRT 格式\n",
        "                sub = pysrt.SubRipItem(\n",
        "                    index=len(subs) + 1,  # 字幕索引\n",
        "                    start=pysrt.SubRipTime.from_ordinal(int(segment.start * 1000)),  # 转换 start 为 SRT 时间格式\n",
        "                    end=pysrt.SubRipTime.from_ordinal(int(segment.end * 1000)),  # 转换 end 为 SRT 时间格式\n",
        "                    text=segment.text  # 字幕内容\n",
        "                )\n",
        "                subs.append(sub)\n",
        "\n",
        "            # 设置输出 SRT 文件路径\n",
        "\n",
        "        srt_path = os.path.join(config[\"asr_path\"], f\"{basename}.srt\")\n",
        "\n",
        "\n",
        "        subs.save(srt_path)\n",
        "        print(f\"字幕写入: {srt_path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6翻译字幕"
      ],
      "metadata": {
        "id": "-fRcCEiH2chU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUCYF8sjWo7z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}