{"cells":[{"cell_type":"markdown","metadata":{"id":"cPPtDIWSblQR"},"source":["#1挂载云盘"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5896,"status":"ok","timestamp":1745776742555,"user":{"displayName":"Joshua","userId":"13780868708307978818"},"user_tz":-480},"id":"tvkI52m5DRsL","outputId":"9f38e2c0-d16f-45be-b41c-180cf431485d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"jus5BKq_bteg"},"source":["#2安装依赖"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQ1Wqb9HdfkG"},"outputs":[],"source":["!pip uninstall torch torchvision torchaudio -y\n","\n","# Workaround from: https://github.com/m-bain/whisperX/issues/1027#issuecomment-2627525081\n","!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n","\n","# WhisperX-related packages:\n","!pip install ctranslate2==4.4.0\n","!pip install faster-whisper==1.1.1\n","\n","!apt-get update\n","!apt-get install libcudnn8=8.9.2.26-1+cuda12.1\n","!apt-get install libcudnn8-dev=8.9.2.26-1+cuda12.1\n","!pip install pyannote.audio\n","\n","!python -c \"import torch; torch.backends.cuda.matmul.allow_tf32 = True; torch.backends.cudnn.allow_tf32 = True\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":562165,"status":"ok","timestamp":1745781009519,"user":{"displayName":"Joshua","userId":"13780868708307978818"},"user_tz":-480},"id":"lB1nRNuqnu9F","outputId":"543cafde-df8c-4af5-d3e3-144ab2f11129"},"outputs":[{"name":"stdout","output_type":"stream","text":["设备: cuda 类型: float16\n","\n","处理音频: /content/gdrive/MyDrive/ASR/1work/MIDV-771.wav\n","字幕写入: /content/gdrive/MyDrive/ASR/1work/MIDV-771.srt\n"]}],"source":["import gc\n","import torch\n","import os\n","import librosa\n","from faster_whisper import WhisperModel\n","\n","# 配置\n","path = '/content/gdrive/MyDrive/ASR'\n","config = {\n","    \"work_path\": \"1work\",\n","    \"asr_path\": \"1work\",\n","    \"model_path\": \"model\",\n","\n","    \"prompt\": \"\",\n","    \"language\": \"ja\",\n","\n","    \"asr\": \"large-v2\",\n","}\n","\n","config[\"work_path\"] = os.path.join(path, config[\"work_path\"])\n","config[\"asr_path\"] = os.path.join(path, config[\"asr_path\"])\n","\n","# 工具函数\n","def timestamp_to_srt(ts: float) -> str:\n","    hours = int(ts // 3600)\n","    minutes = int((ts % 3600) // 60)\n","    seconds = int(ts % 60)\n","    millis = int((ts - int(ts)) * 1000)\n","    return f\"{hours:02}:{minutes:02}:{seconds:02},{millis:03}\"\n","\n","# 硬件\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n","print('设备:', device, '类型:', compute_type)\n","\n","# 初始化ASR模型\n","asr_model = WhisperModel(\n","    config[\"asr\"],\n","    device=device,\n","    compute_type=compute_type,\n","    download_root=config[\"model_path\"]\n",")\n","\n","# 遍历所有音频\n","for root, dirs, files in os.walk(config[\"work_path\"]):\n","    for filename in files:\n","        if not filename.endswith(('.wav', '.mp3')):\n","            continue\n","\n","        audio_path = os.path.join(root, filename)\n","        basename = os.path.splitext(filename)[0]\n","        print(f\"\\n处理音频: {audio_path}\")\n","\n","        # 加载音频\n","        audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        # 直接转写全音频，启用VAD过滤\n","        segments, _ = asr_model.transcribe(\n","            audio=audio,\n","            beam_size=2,\n","            vad_filter=True,\n","            initial_prompt=basename,\n","            language=config['language'],\n","            word_timestamps=False\n","        )\n","\n","        # 写出SRT文件\n","        os.makedirs(config[\"asr_path\"], exist_ok=True)\n","        srt_path = os.path.join(config[\"asr_path\"], f\"{basename}.srt\")\n","\n","        with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n","            for idx, seg in enumerate(segments, start=1):\n","                start_time = timestamp_to_srt(seg.start)\n","                end_time = timestamp_to_srt(seg.end)\n","                text = seg.text.strip()\n","\n","                f.write(f\"{idx}\\n\")\n","                f.write(f\"{start_time} --> {end_time}\\n\")\n","                f.write(f\"{text}\\n\\n\")\n","        print(f\"字幕写入: {srt_path}\")\n","\n","        # 释放内存\n","        del audio, segments\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","# 最后释放模型\n","del asr_model\n","gc.collect()\n","torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","metadata":{"id":"X69e__kob7aA"},"source":["#5转写音频"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"9FLArhie45Cu","outputId":"e47d2b54-5a6f-4105-ec38-b5f8dc6b9811"},"outputs":[{"name":"stderr","output_type":"stream","text":["DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"]},{"name":"stdout","output_type":"stream","text":["设备: cuda 类型: float16\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","处理音频: /content/gdrive/MyDrive/ASR/1work/MIDV-771.wav\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n","It can be re-enabled by calling\n","   >>> import torch\n","   >>> torch.backends.cuda.matmul.allow_tf32 = True\n","   >>> torch.backends.cudnn.allow_tf32 = True\n","See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n","\n","  warnings.warn(\n"]}],"source":["import gc\n","import torch\n","import numpy as np\n","from pyannote.audio import Model\n","import os\n","from faster_whisper import WhisperModel\n","import librosa\n","from pyannote.audio.pipelines import VoiceActivityDetection\n","from dataclasses import dataclass\n","\n","# 配置\n","path = '/content/gdrive/MyDrive/ASR'\n","config = {\n","    \"work_path\": \"1work\",\n","    \"asr_path\": \"1work\",\n","    \"log_path\": \"log\",\n","    \"model_path\": \"model\",\n","\n","    \"prompt\": \"\",\n","    \"language\": \"ja\",\n","    \"space\": 3,\n","    \"min_duration_on\": 0.0,\n","    \"min_duration_off\": 0.2,\n","\n","    \"asr\": \"large-v2\",\n","    \"vad\": \"4evergr8/pyannote-segmentation-3.0\",\n","\n","    \"output\": [\"lrc\", \"srt\", \"vtt\"]\n","}\n","\n","config[\"work_path\"] = os.path.join(path, config[\"work_path\"])\n","config[\"asr_path\"] = os.path.join(path, config[\"asr_path\"])\n","\n","# 数据结构\n","@dataclass\n","class AudioSegmentInfo:\n","    start_in_group: float\n","    end_in_group: float\n","    start_in_origin: float\n","    end_in_origin: float\n","\n","@dataclass\n","class AudioSegmentGroup:\n","    audio: np.ndarray\n","    segments: list\n","    offset: float\n","\n","# 工具函数\n","def timestamp_to_srt(ts: float) -> str:\n","    hours = int(ts // 3600)\n","    minutes = int((ts % 3600) // 60)\n","    seconds = int(ts % 60)\n","    millis = int((ts - int(ts)) * 1000)\n","    return f\"{hours:02}:{minutes:02}:{seconds:02},{millis:03}\"\n","\n","# 硬件\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n","print('设备:', device, '类型:', compute_type)\n","\n","# 初始化ASR模型\n","asr_model = WhisperModel(\n","    config[\"asr\"],\n","    device=device,\n","    compute_type=compute_type,\n","    download_root=config[\"model_path\"]\n",")\n","\n","# 遍历所有音频\n","for root, dirs, files in os.walk(config[\"work_path\"]):\n","    for filename in files:\n","        if not filename.endswith(('.wav', '.mp3')):\n","            continue\n","\n","        audio_path = os.path.join(root, filename)\n","        basename = os.path.splitext(filename)[0]\n","        print(f\"\\n处理音频: {audio_path}\")\n","\n","        # Step 1: 加载音频\n","        audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        vad_model = Model.from_pretrained(checkpoint=config[\"vad\"], cache_dir=config[\"model_path\"])\n","        vad_model.to(torch.device(device))\n","        vad_pipeline = VoiceActivityDetection(segmentation=vad_model)\n","        vad_pipeline.instantiate({\n","            \"min_duration_on\": config[\"min_duration_on\"],\n","            \"min_duration_off\": config[\"min_duration_off\"],\n","        })\n","\n","        vad_result = vad_pipeline(str(audio_path))\n","\n","        # VAD结束，释放模型\n","        del vad_pipeline, vad_model\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        # Step 3: 切人声片段，并按30分钟分组\n","        timeline = vad_result.get_timeline()\n","        silence = np.zeros(int(sr * config[\"space\"]), dtype=audio.dtype)\n","\n","        groups = []\n","        current_audio = []\n","        current_segments = []\n","        current_time_concat = 0.0\n","        current_group_start = 0.0\n","        group_time_limit = 30 * 60\n","\n","        for segment in timeline:\n","            if segment.start >= current_group_start + group_time_limit:\n","                if current_segments:\n","                    final_audio = np.concatenate(current_audio[:-1])\n","                    groups.append(AudioSegmentGroup(audio=final_audio, segments=current_segments, offset=current_group_start))\n","\n","                current_audio = []\n","                current_segments = []\n","                current_time_concat = 0.0\n","                current_group_start += group_time_limit\n","\n","            start_sample = int(segment.start * sr)\n","            end_sample = int(segment.end * sr)\n","\n","            audio_seg = audio[start_sample:end_sample]\n","            duration = (end_sample - start_sample) / sr\n","\n","            segment_info = AudioSegmentInfo(\n","                start_in_group=current_time_concat,\n","                end_in_group=current_time_concat + duration,\n","                start_in_origin=segment.start,\n","                end_in_origin=segment.end\n","            )\n","\n","            current_segments.append(segment_info)\n","            current_audio.append(audio_seg)\n","            current_audio.append(silence)\n","            current_time_concat += duration + config[\"space\"]\n","\n","        if current_segments:\n","            final_audio = np.concatenate(current_audio[:-1])\n","            groups.append(AudioSegmentGroup(audio=final_audio, segments=current_segments, offset=current_group_start))\n","\n","        if not groups:\n","            print(f\"{audio_path} 没有有效人声段，跳过\")\n","            continue\n","\n","        # Step 4: ASR转写 + 时间戳匹配\n","        srt_segments = []\n","        subtitle_index = 1\n","\n","        for group in groups:\n","            segments, _ = asr_model.transcribe(\n","                audio=group.audio,\n","                beam_size=2,\n","                vad_filter=False,\n","                initial_prompt=basename,\n","                language=config['language'],\n","                word_timestamps=False\n","            )\n","\n","            asr_results = []\n","            for seg in segments:\n","                asr_results.append((seg.start, seg.end, seg.text.strip()))\n","\n","            for segment_info in group.segments:\n","                found_text = \"...\"\n","\n","                for asr_start, asr_end, asr_text in asr_results:\n","                    latest_start = max(segment_info.start_in_group, asr_start)\n","                    earliest_end = min(segment_info.end_in_group, asr_end)\n","                    overlap = max(0.0, earliest_end - latest_start)\n","\n","                    if overlap > 0:\n","                        found_text = asr_text\n","                        break\n","\n","                srt_segments.append((\n","                    subtitle_index,\n","                    segment_info.start_in_origin,\n","                    segment_info.end_in_origin,\n","                    found_text\n","                ))\n","                subtitle_index += 1\n","\n","        # Step 5: 写出SRT文件\n","        os.makedirs(config[\"asr_path\"], exist_ok=True)\n","        srt_path = os.path.join(config[\"asr_path\"], f\"{basename}.srt\")\n","        with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n","            for idx, start, end, text in srt_segments:\n","                f.write(f\"{idx}\\n\")\n","                f.write(f\"{timestamp_to_srt(start)} --> {timestamp_to_srt(end)}\\n\")\n","                f.write(f\"{text}\\n\\n\")\n","        print(f\"字幕写入: {srt_path}\")\n","\n","        # 回收\n","        del audio, groups, timeline, srt_segments\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","# 释放ASR模型\n","del asr_model\n","gc.collect()\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1745776660937,"user":{"displayName":"Joshua","userId":"13780868708307978818"},"user_tz":-480},"id":"XUCYF8sjWo7z","outputId":"4e7ca41a-3c9c-425e-8328-bdf5be234f8c"},"outputs":[{"data":{"text/plain":["<function posix.kill(pid, signal, /)>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.kill"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}